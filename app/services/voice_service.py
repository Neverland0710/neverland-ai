# app/services/voice_service.py
import asyncio
import aiohttp
from typing import AsyncGenerator, Dict, Any
from datetime import datetime

from app.services.database_service import database_service
from app.chains.voice_chain import voice_chain
from app.config import settings
from app.utils.logger import logger

class VoiceService:
    def __init__(self):
        self.voice_chain = voice_chain
        self.db_service = database_service
        self.elevenlabs_api_key = settings.elevenlabs_api_key

    def _detect_user_emotion(self, user_text: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê°ì • ê°ì§€"""
        user_text_lower = user_text.lower()
        
        # ìŠ¬í”ˆ ê°ì • í‚¤ì›Œë“œ
        sad_keywords = ["í˜ë“¤ì–´", "ìŠ¬í¼", "ìš°ìš¸í•´", "ì•„íŒŒ", "ê·¸ë¦¬ì›Œ", "ë³´ê³ ì‹¶ì–´", "ëˆˆë¬¼", "ìš¸ê³ ", "ì™¸ë¡œì›Œ", "ê³ ë¯¼", "ê±±ì •"]
        if any(keyword in user_text_lower for keyword in sad_keywords):
            return "sad"
        
        # ê¸°ìœ ê°ì • í‚¤ì›Œë“œ  
        happy_keywords = ["ê¸°ë»", "ì¢‹ì•„", "í–‰ë³µí•´", "ì¦ê±°ì›Œ", "ì›ƒìŒ", "ì¬ë¯¸ìˆì–´", "ì‹ ë‚˜", "ì¶•í•˜", "ì„±ê³µ", "í•©ê²©"]
        if any(keyword in user_text_lower for keyword in happy_keywords):
            return "happy"
        
        # í™”ë‚œ ê°ì • í‚¤ì›Œë“œ
        angry_keywords = ["í™”ë‚˜", "ì§œì¦", "ë‹µë‹µí•´", "ë¯¸ì³", "ì—´ë°›ì•„", "ì–µìš¸í•´", "ë¹¡ì³", "ìŠ¤íŠ¸ë ˆìŠ¤"]
        if any(keyword in user_text_lower for keyword in angry_keywords):
            return "empathetic"  # í™”ë‚  ë•ŒëŠ” ê³µê°ì ìœ¼ë¡œ ëŒ€ì‘
        
        # ì°¨ë¶„í•œ/ì•ˆì • í‚¤ì›Œë“œ
        calm_keywords = ["í‰ì˜¨", "ì°¨ë¶„", "ì•ˆì •", "ì¡°ìš©", "í¸ì•ˆ", "íœ´ì‹", "ëª…ìƒ"]
        if any(keyword in user_text_lower for keyword in calm_keywords):
            return "calm"
        
        return "neutral"

    async def generate_response_and_voice(
        self,
        user_text: str,
        user_info: dict,
        authKeyId: str,
        voice_emotion: str = None  # Noneì´ë©´ ìë™ ê°ì§€
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± + ìŒì„± ìƒì„±ê¹Œì§€ í•œë²ˆì—"""

        logger.info(f"ğŸ¤ [í†µí•© ì²˜ë¦¬] í…ìŠ¤íŠ¸+ìŒì„± ìƒì„± ì‹œì‘")

        # âœ… 1. ê³ ì¸ ì •ë³´ ì¡°íšŒ
        deceased_info = await self.db_service.get_deceased_by_auth_key(authKeyId)
        if not deceased_info:
            raise Exception("ê³ ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        voice_id = deceased_info.get("voice_id") or getattr(settings, 'default_voice_id', 'DMkRitQrfpiddSQT5adl')

        # âœ… 2. ê°ì • ê°ì§€ (voice_emotionì´ Noneì´ë©´ ìë™ ê°ì§€)
        if voice_emotion is None:
            detected_emotion = self._detect_user_emotion(user_text)
            logger.info(f"ğŸ­ ê°ì • ìë™ ê°ì§€: '{user_text}' -> {detected_emotion}")
        else:
            detected_emotion = voice_emotion
            logger.info(f"ğŸ­ ê°ì • ìˆ˜ë™ ì„¤ì •: {detected_emotion}")

        # âœ… 3. GPT ì‘ë‹µ ìƒì„± (ê°ì • ë°˜ì˜)
        voice_result = await self.voice_chain.generate_voice_response(
            user_speech_text=user_text,
            user_id=user_info.get("user_id"),
            authKeyId=authKeyId,
            voice_emotion=detected_emotion
        )

        if voice_result["status"] != "success":
            raise Exception("GPT ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")

        # âœ… 4. í…ìŠ¤íŠ¸ â†’ TTSë¡œ ë³€í™˜ (ì•ˆì •ì ì¸ ì„¤ì •ìœ¼ë¡œ)
        gpt_text = voice_result["voice_response"]
        
        audio_data = b""
        async for audio_chunk in self._stream_elevenlabs_tts_http(gpt_text, voice_id):
            if audio_chunk:
                audio_data += audio_chunk

        logger.info(f"âœ… [í†µí•© ì™„ë£Œ] í…ìŠ¤íŠ¸ + TTS ë³€í™˜ ì™„ë£Œ ({len(audio_data)} bytes, emotion: {detected_emotion})")

        return {
            "response_text": gpt_text,
            "audio_mp3": audio_data,
            "voice_analysis": voice_result.get("voice_analysis"),
            "emotion_risk": voice_result.get("emotion_risk", "LOW"),
            "used_memories": voice_result.get("used_memories", []),
            "detected_emotion": detected_emotion,  # ê°ì§€ëœ ê°ì • ì¶”ê°€
            "status": "success"
        }

    async def _stream_elevenlabs_tts_http(self, text: str, voice_id: str) -> AsyncGenerator[bytes, None]:
        """HTTP API ë°©ì‹ TTS - ì•ˆì •ì ì¸ ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©"""
        try:
            logger.info(f"ğŸ”Š HTTP TTS ì‹œì‘: voice_id={voice_id}")

            url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream"
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": self.elevenlabs_api_key
            }

            # ì•ˆì •ì ì¸ ê¸°ë³¸ ì„¤ì •ë§Œ ì‚¬ìš©
            data = {
                "text": text,
                "model_id": "eleven_multilingual_v2",
                "voice_settings": {
                    "stability": 0.8,
                    "similarity_boost": 0.9,
                    "style": 0.5,  # ê³ ì •ê°’
                    "use_speaker_boost": True
                }
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=data) as response:
                    if response.status == 200:
                        async for chunk in response.content.iter_chunked(8192):
                            if chunk:
                                yield chunk
                        logger.info("âœ… HTTP TTS ì™„ë£Œ")
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ HTTP TTS ì‹¤íŒ¨: {response.status} - {error_text}")

        except Exception as e:
            logger.error(f"âŒ HTTP TTS ì‹¤íŒ¨: {str(e)}")